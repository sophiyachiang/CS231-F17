\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{enumitem}

\usetikzlibrary{automata,positioning}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass\ : \hmwkTitle}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}


\newcommand{\hmwkTitle}{Reading Notes}
\newcommand{\hmwkClass}{CS231: Fundamentals of Algorithms}
\newcommand{\hmwkAuthorName}{\textbf{Sophiya Chiang}}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Reading \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Reading \arabic{#1} (continued)}{Reading \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Reading \arabic{#1} (continued)}{Reading \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Reading \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Reading \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Last updated: 20 September 2017}\\
    \vspace{3in}
}

\author{\hmwkAuthorName}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}


\begin{document}

\maketitle

\pagebreak

%Reading for September 11, 2017
\begin{homeworkProblem}
\textbf{Tractability and Asymptotic order of growth}\\
\textbf{Sections 2.1 and 2.2}\\\\
\underline{\textbf{2.1 Computational Tractability}}
\begin{itemize}
\item
\textit{Worst-case} running time: A bound on the largest possible running time the algorithm could have over all inputs of a given size $N$
\item An algorithm is efficient if it achieves qualitatively better worst-case performances, at an analytical level, than brute-force search.
\item An algorithm is efficient if it has a polynomial running time: lower-degree polynomials exhibit better scaling behavior than higher-degree polynomials.
\end{itemize}

\underline{\textbf{2.2 Asymptotic Order of Growth}}
\begin{itemize}
\item An algorithm's worst-case running time on inputs of size $n$ grows at a rate that is at most proportional to some function $f(n)$. The function $f(n)$ becomes a bound on the running time of the algorithm.
\item
\textit{Asymptotic upper bounds:} $T(n)$ is $O(n)$ ($T(n)$ is order of $n$) if, for sufficiently large $n$ the function $T(n)$ is bounded above by a constant multiple of $f(n)$. So, $T(n)$ is \textit{asymptotically upper-bounded by $f(n)$}.
\begin{center}
    $T(n) \in O(n)$ if $\exists (c > 0$ and $n_0 \geq 0)$ $\mid$ \ $\forall n \geq n_0, T(n) \leq c \cdot f(n)$
\end{center}

\item
\textit{Asymptotic lower bounds:} $T(n)$ is $\Omega(n)$ if, for arbitrarily large input size $n$, the function $T(n)$ is at least a constant multiple of $f(n)$. So, $T(n)$ is \textit{asymptotically lower-bounded by $f(n)$}.
\begin{center}
    $T(n) \in O(n)$ if $\exists (\epsilon > 0$ and $n_0 \geq 0)$ $\mid$ \ $\forall n \geq n_0, T(n) \geq \epsilon \cdot f(n)$
\end{center}

\item
\textit{Asymptotically tight bounds:} If a function is both $O(n)$ and $\Omega(n)$, then $T(n)$ is $\Theta(n)$. They characterize the worse-case performance of an algorithm precisely up to constant factors. So, $f(n)$ is an \textit{asymptotically tight bound for $T(n)$}. One can sometimes obtain an asymptotically tight bound directly by computing the limit as $n$ goes to infinity. If the ratio of functions $f(n)$ and $g(n)$ converges to a positive constant as $n$ goes to infinity, then $f(n) \in \Theta(g(n))$
\begin{center}
    $T(n) \in O(n)$ if $\exists (\epsilon > 0$ and $n_0 \geq 0)$ $\mid$ \ $\forall n \geq n_0, T(n) \geq \epsilon \cdot f(n)$
\end{center}

\item 
\textbf{Properties of Asymptotic Growth Rates:}
\textit{Transitivity:} If a function $f$ is asymptotically upper-bounded by a function $g$, and $g$ is asymptotically upper-bounded by $h$, then $f$ is asymptotically upper-bounded by $h$. 
\begin{enumerate}
\item
If $f = O(g)$ and $g = O(h)$, then $f = O(h)$.
\item
If $f = \Omega(g)$ and $g = \Omega(h)$, then $f = \Omega(h)$.
\item 
If $f = \Theta(g)$ and $g = \Theta(h)$, then $f = \Theta(h)$.
\item
Suppose that $f$ and $g$ are two functions such that for some other function $h$, we have $f=O(h)$ and $g=O(h)$. Then, $f + g = O(h)$.
\item 
Let $k$ be  fixed constant, and let $f_1, f_2,\ldots, f_k$ and $h$ be functions such that $f_i = O(h) \forall i$. Then, $f_1 + f_2+\ldots+ f_k = O(h)$.
\item
Suppose that $f$ and $g$ are two functions (taking non-negative values) such that for $g = O(f)$. Then, $f + g = \Theta(f)$. $f$ is an asymptotically tight bound for the combined function $f + g$.

\end{enumerate}
\end{itemize}
\end{homeworkProblem}
\pagebreak

%Reading for September 14, 2017
\begin{homeworkProblem}
\textbf{The Stable Matching Problem}\\
\textbf{Sections 2.3}\\\\
\underline{\textbf{2.3 Implementing the Stable Matching Algorithm Using Lists and Arrays}}
\begin{itemize}
\item
Consider how the data will be represented and manipulated in an implementation of the algorithm, so as to bound the number of computational steps it takes.
\item
Considering the Gale-Shapely Matching algorithm, the algorithm terminates in at most $n^2$ iterations and to get an implementation with worst-case running time of $O(n^2)$, we need to be able to implement each iteration in constant time by counting actual computational steps rather than simply the total number of iterations using arrays and lists.
\item
We need to be able to do each of these four steps of the algorithm in constant time:
\begin{enumerate}
    \item Identify a free man. \textbf{Implemented in $O(1)$ time}
        \begin{itemize}
            \item Maintaining the set of free men as a linked list.
            \item When selecting a free man, we take the first man $m$ on this list. 
            \item We delete $m$ from the list if he becomes engaged, and possibly insert a different man $m'$, if some other man $m'$ becomes free. $m'$ can be inserted at the front of the list at constant time in this case.
        \end{itemize}
    \item For a man $m$, we need to identify the highest-ranked woman to whom he has not yet proposed. \textbf{Implemented in $O(1)$ time}
        \begin{itemize}
        \item 
        Maintain an extra array $Next$ that indicates for each man $m$ the position of the next woman he will propose to on his list.
        \item We initialize $Next[m] = 1$ for all men $m$. If a man $m$ needs to propose to a woman, he'll propose to $w = ManPref[m, Next[m]]$
        \item Once he proposes to $w$, we increment the value of $Next[m]$ by one, regardless of whether $w$ accepts the proposal.
        \end{itemize}
    \item For a woman $w$, we need to decide if $w$ is currently engaged, and if she is, we need to identify her current partner. \textbf{Implemented in $O(1)$ time}
        \begin{itemize}
            \item Assuming man $m$ proposes to woman $w$, we need to identify the man $m'$ that $w$ is currently engaged to (if exists).
            \item Maintain an array $Current$ of length $n$, where $Current[w]$ is the woman $w$'s current partner $m'$.
            \item We set $Current[w]$ to a special null value when woman $w$ is not currently engaged.
            \item Initialize $Current[w]$ to null value at the start of the algorithm.
        \end{itemize}
    
    \item For a woman $w$, and two men $m$ and $m'$, we need to be able to decide in constant time, which of $m$ or $m'$ is preferred by $w$. \textbf{Implemented in $O(n^2)$ time}
        \begin{itemize}
            \item At the start of algorithm, we create an $n\times n$ array $Ranking$, where $Ranking[w,m]$ contains the rank of man $m$ in the sorted order of $w$'s preferences. 
            \item By a single pass through $w$'s preference list, we can create this array in linear time for each woman, for a total initial time investment proportional to $n^2$.
            \item To decide which of $m$ or $m'$ is preferred by $w$, we simply compare values $Ranking[w,m]$ to $Ranking[w,m']$.
        \end{itemize}
\end{enumerate}
\end{itemize}
\end{homeworkProblem}
\pagebreak

%Reading for September 18, 2017
\begin{homeworkProblem}
\textbf{Priority Queues}\\
\textbf{Sections 2.5}\\\\
\underline{\textbf{2.5 Priority Queues}}
\begin{itemize}
    \item For the Stable Matching algorithm, we need to maintain a dynamically changing set $S$ (set of free men in this case).
    \item We want to be able to add and delete elements from the set $S$, and we want to be able to select an element $S$ when the algorithm calls for it.
    \item A priority queue is designed for applications in which elements have a \textit{priority value (key)} and each time we need to select an element from $S$, we want to take the one with hightest priority.
    \item Maintains a set of elements $S$, where element $v \in S$ has an associated value $key(v)$ that denotes the priority of element $v$, where smaller keys represent higher priorities.
    \item Supports the addition and deletion of elements from the set, and also the selection of the element with smallest key. 
    \item Each process has a priority/urgency, but processes do not arrive in order of their priorities.
    \item Implementation of a priority queue containing at most $n$ elements at any time so that elements can be added and deleted, and the element with minimum key selected, in $O(log (n))$ time per operation.
    \item \textit{A sequence of $O(n)$ priority queue operations can be used to sort a set of $n$ numbers}\\
    \textbf{Proof: }Set up a priority queue $H$, and insert each number into $H$ with its value as a key. Then extract the smallest number one by one until all numbers have been extracted. This way, the numbers will come out of the priority queue in sorted order.
    \item So with a priority queue that can perform insertion and deletion of minima $O(log(n))$ time per operation, we can sort $n$ numbers in $O(n(log(n)))$ time.
    
    \item\textbf{Heap} - Implementing a Priority Queue
    \begin{itemize}
        \item The heap data structure combines the benefits of a sorted array and list for purposes of this application.
        \item Conceptually, we can think of it as a balanced binary tree. Where the tree will have a root, and each node can have up to two children (A left and a right child).
        \item \textit{Heap order: }If the key of any element is at least as large as the key of the element at its parent node in the tree.
        \begin{center}
        For every element $v$, at a node $i$, the element $w$ at $i$'s parent satisfies $key(w) \leq key(i)$.
        \end{center}
        \item If a bound $N$ is known in advance on the total number of elements that will ever be in the heap at any one time, we can maintain the heap in an array $H$ indexed by $i=1, 2, \ldots, N$.
        \item We can think of heap nodes as corresponding to the positions of the array, with $H[1]$ being the root.
        \item For any node at position $i$, the children are the nodes at position $leftChild(i) = 2i$ and $rightChild(i) = 2i+1$. And the parent of a node at position $i$ is at position $parent(i) = i/2$.
        \item If the heap has $n<N$ elements at some time, we can use the first $n$ positions of the array to store the $n$ heap elements. Using $length(H)$ to denote the number of elements in $H$. So the heap is balanced at all times.
    \end{itemize}
    \item\textbf{Implementing the Heap Operations:}
    \begin{itemize}
        \item The heap element with smallest key is at the root, so it takes $O(1)$ time to identify the minimal element. 
        \item Adding element $v$ to final position $i = n + 1$ and fixing the heap with \textbf{Heapify-up} by moving element $v$ towards the root. 
        \item The \textbf{Heapify-up} procedure fixes the heap property in $O(log(n))$ time, assuming that the array $H$ is almost a heap with the key $H[i]$ too small. Using the following procedure, we can insert a new element in a heap of $n$ elements in $O(log(n))$ time.
        \begin{lstlisting}
        Heapify-up(H,i):
            If i > 1 then
                let j = parent(i) = (1/2)
                If key[H[i]] < key[H[j]] then
                    swap the array entries H[i] and H[j]
                    Heapify-up(H,j)
                Endif
            Endif
        \end{lstlisting}
        Proof by induction on pp.62 of textbook.
        \item Procedure \textbf{Heapify-down} swaps the element at position $i$ with one of its children and proceeds down the tree recursively.
        \item \textbf{Heapify-down(H,i)} fixes the heap property in $O(log(n))$ time, assuming that $H$ is almost a heap with the key value of $H[i]$ too big. Using \textbf{Heapify-up} or \textbf{Heapify-down}, we can delete a new element in a heap of n elements in $O(log(n))$ time.
        \begin{lstlisting}
        Heapify-down(H,i):
            Let n = length(H)
            If 2i > n then
                Terminate with H unchanged
            Else if 2i < n then
                Let left = 2i, and right = 2i + 1
                Let j be the index that minimizes key[H[left]] and key[H[right]]
            Else if 2i = n then
                Let j = 2i
            Endif
            If key[H[j]] < key[H[i]] then
                Swap the array entries H[i] and H[j]
                Heapify-down[H, j]
            Endif
        \end{lstlisting}
        Proof by reverse induction on value i on pp.64 of textbook.
    \end{itemize}
    \item\textbf{Implementing Priority Queues with Heaps}\\
    If the priority queue is constrained to hold at most $N$ elements at any point in time.
    \begin{itemize}
        \item $StartHeap(N)$ returns an empty heap $H$ that is set up to store at most $N$ elements. This operation takes $O(n)$ time, as it involves initializing the array that will hold the heap.
        \item $Insert(H,v)$ inserts the item $v$ into the heap $H$. If the heap currently has elements, this takes $O(log(n))$ time.
        \item $FindMin(H)$ identifies the minimum element in the heap $H$ but does not remove it. This takes $O(1)$ time.
        \item $Delete(H,i)$ deletes the element in heap position $i$. This is implements in $O(log(n)$ time for heaps that have $n$ elements.
        \item $ExtractMin(H)$ identifies and deletes and element with minimum key value from a heap. This is a combination of the previous two operations, so this takes $O(log(n))$ time.
    \end{itemize}
    To be able to access given elements of the priority queue efficiently, we simply maintain an additional array $Position$ that stores the current position of each element (each node) in the heap.
    \begin{itemize}
        \item To delete the element $v$, we apply $Delete(H, Position[v])$. Maintaining this array does not increase the overall running time, so we can delete an element $v$ from a heap with $n$ notes in $O(log(n))$ time.
        \item $ChangeKey(H,v,\alpha)$ which changes the key value of element $v$ to $key(v) = \alpha$. We can implement this operation in $O(log(n))$ time by first identifying the position of element $v$ in the array by using the $Position$ array. Once we have defined the position of element $v$, we change the key and then apply \textbf{Heapify-up} or \textbf{Heapify-down} as appropriate.
    \end{itemize}
\end{itemize}
\end{homeworkProblem}
\pagebreak

\begin{homeworkProblem}
\textbf{Graphs}\\
\textbf{Sections 3.1 and 3.2}\\\\
\underline{\textbf{3.1 Basic Definitions}}
\begin{itemize}

    \item A graph G encodes a pairwise relationship among objects: a collection $V$ of \textit{nodes} and a collection $E$ of \textit{edges}.
    \item An edge $e \in E$ is a 2-element subset of $V: e = \{u,v\}$ for some $u, v \in V$ where $u, v$ are the \textit{ends} of $e$.
    \item Undirected graphs: indicate a symmetric relationship between the ends of the edges in a graph.
    \item Directed graphs: consists of a set of nodes $V$ and a set of \textit{directed edges} $E'$, where each $e' \in E'$ is an ordered pair $(u,v)$.
    \item A path in an undirected graph $G=(V,E)$ is a sequence $P$ of nodes $v_1, v_2, v_3, \ldots, v_{k-1}, v_k$ such that each consecutive pair $v_i, v_{i+1}$ is joined by an edge in $G$.
        \begin{itemize}
            \item Simple path: if all vertices are distinct from one another
            \item Cycle: is a path $v_1, v_2, v_3, \ldots, v_{k-1}, v_k$ where $k>2$, the first $k-1$ nodes are distinct, and $v_1 = v_k$.
        \end{itemize}
    \item Connected graph: if for every pair of nodes $u, v$ there is a path from $u$ to $v$.
    \item Distance between two nodes $u, v$: the minimum number of edges in a $u-v$ path.
    \item Tree: An undirected graph that is connected and does not contain a cycle.
    \item Every $n-node$ tree has exactly $n-1$ edges.
    \item Let $G$ be an undirected graph on $n$ nodes. Any of two of the following statements implies the third.
    \begin{itemize}
        \item G is connected.
        \item G does not contain a cycle.
        \item G has $n-1$ edges.
    \end{itemize}
\end{itemize}

\underline{\textbf{3.2 Graph Connectivity and Graph Traversal}}
\begin{itemize}
    \item $s-t$ connectivity problem/Maze solving problem: finding a path, if any, from $s$ to $t$ in $G$.
    \item Breadth-first search (BFS)
    \begin{itemize}
        \item Explore outward from $s$ in all possible directions, including nodes that are joined by an edge to $s$. Continue until no new nodes are encountered.
        \item Layers $L_1, L_2, L_3,\ldots$: 
        \begin{itemize}
            \item Layer $L_0$ is the set consisting just of $s$
            \item Layer $L_1$ consists of al nodes that are neighbors of $s$.
            \item Assuming we have defined layers $L_1, \ldots, L_j$, then layer $L_{j+1}$ consists of all nodes that do not belong to an earlier later and that have an edge to a node in layer $L_j$.
            \item A node fails to appear in any of the laters iff there is no path to it.
        \end{itemize}
        \item BFS determines the ndoes that $s$ can reach and computes the shortest paths to them.
        \item For each $j \geq 1$, Layer $L_j$ produced by BFS consists of all nodes at distance exactly $j$ from $s$. THere is a path from $s$ to $t$ iff $t$ appears in some layer.
        \item \textit{Breadth-first search tree}: BFS produces a tree $T$ rooted at $s$ on the set of nodes reachable from $s$: Let $T$ be a breadth-first search tree, let $x$ and $y$ be nodes in $T$ belonging to layers $L_i$ and $L_j$ respectively, and let $(x, y)$ be an edge of $G$. Then $i$ and $j$ differ at most by 1.
    \end{itemize}
    \item Depth-first search (DFS)
    \begin{itemize}
        \item We can invoke DFS from any starting point but maintain global knowledge of which nodes have already been explored.
        \item Let $T$ be a depth-first search tree, let $x$ and $y$ be nodes in $T$, and let $(x, y)$ be an edge of $G$ that is not an edge of $T$. Then one of $x$ or $y$ is an ancestor of the other.
    \end{itemize}
    \item For any two nodes $s$ and $t$ in a graph, their connected components are either identical or disjoint.
\end{itemize}

\end{homeworkProblem}
\pagebreak

\begin{homeworkProblem}
\textbf{Graph traversal}\\
\textbf{Sections 3.3 and 3.4}\\\\
\underline{\textbf{3.3 Implementing Graph Traversal Using Queues and Stacks}}
\begin{itemize}
    \item Basic representation of graphs: By an \textit{adjacency matrix} and by an \textit{adjacency list}.
    \item A graph $G=(V,E)$, let $|V| = n$ and $|E| = m$. With at most one edge between any pair of nodes, $m \leq \binom{n}{2} \leq n^2$.
    \item A connected graph must have at least $m \geq n-1$ edges.
    \item Linear time for graph search: $O(m+n)$ time to read the input.
    \item For connected graphs: $O(m+n) \equiv O(m)$ since $m\geq n-1$.
    \item\textbf{Adjacency matrix:} Consider a graph $G = (V , E)$ with $n$ nodes, and assume the set of nodes is $V = \{1,\ldots, n\}$. 
    \begin{itemize}
        \item The adjacency matrix is an $n\times n$ matrix $A$ where $A[u,v]$. is equal to 1 if the graph contains the edge $(u,v)$ and 0 otherwise.
        \item If the graph is undirected, the matrix $A$ is symmetric, with $A[u,v] = A[v,u]$ for all nodes $u, v \in V$.
        \item Can check in $O(1)$ time if a given edge $(u,v)$ is present in the graph.
        \item Representation takes $\Theta(n^2)$ space. When the graph has many fewer edges than $n^2$, more compact representations are possible.
        \item Many graph algorithms need to examine all edges incident to a given node $v$ and in the matrix, you have to consider all other nodes $w$, and checking the matrix entry $A[v, w]$ to see whether the edge $(v, w)$ is present in $\Theta(n)$ time. Worst case, $v$ may have $\Theta(n)$ incident edges, so checking all edges will take $\Theta(n)$ time regardless of the representation. 
    \end{itemize}
    \item\textbf{Adjacency list:} Array $adj$ where $Adj[v]$ is a record containing a list of all nodes adjacent to node $v$. 
    \begin{itemize}
        \item Works better for sparse graphs with fewer than $n^2$ edges.
        \item For an undirected graph $G = (V, E)$, each edge $e = (v, w) \in E$ occurs on two adjacency lists: node w appears on the list for node $v$, and node $v$ appears on the list for node $w$.
        \item Requires only $O(m+n)$ space: array of pointers of length $n$ to set up the lists in $Adj$, and then we need space for all the lists - each edge $e = (v,w)$ appears in exactly two of the lists: one for $v$ and one for $w$. So the total length of all lists is $2m = O(m)$.
        \item Degree $n_v$ of a node $v$ is the number of incident edges it has.
        \item Length of the list at $Adj[v]$ is $n_v$, so the total length over all nodes is $O(\sum_{v\in V} n_v)$.
        \item Sum of the degrees in a graph: $O(\sum_{v\in V} n_v) = 2m$.
        \item To check if a particular edge $(u,v)$ is present in the graph: in $O(n_v)$ - by following the pointers on $u$'s adjacency list to see if edge $v$ occurs on the list.
    \end{itemize}

    \item\textit{Discovering} a node $v$ - the first time it is seen, when the algorithm finds an edge leading to $v$.
    \item\textit{Exploring} a node $v$ - when all the incident edges to $v$ are scanned, resulting in the potential discovery of further nodes.
    \item Implement queues and stacks using doubly-linked list - always selecting the first element on our list, different in where we insert a new element.
    \item\textbf{Queue: }A set from which we extract elements in first-in, first-out order (FIFO) - select elements in the same order in which they were added. Elements are added to the end of the list as the last element. \textit{Implement BFS to select which node to consider next.}
    \begin{itemize}
        \item Implementing BFS:
        \begin{lstlisting}
BFS(s):
    Set Discovered[s] = true and Discovered[v] = false for all other v 
    Initialize L[0] to consist of the single element s
    Set the layer counter i = 0
    Set the current BFS tree T = ∅
    While L[i] is not empty
        Initialize an empty list L[i + 1] 
        For each node u ∈ L[i]
            Consider each edge (u, v) incident to u 
            If Discovered[v] = false then
                Set Discovered[v] = true
                Add edge (u, v) to the tree T
                Add v to the list L[i+1] 
            Endif
        Endfor
        Increment the layer counter i by one 
    Endwhile
        \end{lstlisting}
        \item The above implementation of the BFS algorithm runs in time $O(m + n)$ (i.e., linear in the input size), if the graph is given by the adjacency list representation.
    \end{itemize}
    
    \item\textbf{Stack: }A set from which we extract elements in last-in, first-out order (LIFO) - we select an element that was added most recently. A new element is placed in the first position on the list. \textit{Implement DFS.}
    \begin{itemize}
        \item Implementing DFS:
        \begin{lstlisting}
DFS(s):
    Initialize S to be a stack with one element s
    While S is not empty
        Take a node u from S
        If Explored[u] = false then
            Set Explored[u] = true
            For each edge (u,v) incident to u
                Add v to the stack S
            Endfor
        Endif
    Endwhile
        \end{lstlisting}
        \item The above algorithm implements DFS, in the sense that it visits the nodes in exactly the same order as the recursive DFS procedure in the previous section (except that each adjacency list is processed in reverse order).
        \item The above implementation of the DFS algorithm runs in time $O(m + n)$ (i.e., linear in the input size), if the graph is given by the adjacency list representation.
    \end{itemize}
\end{itemize}
\pagebreak

\underline{\textbf{3.4 Testing Bipartite-ness}}
\begin{itemize}
\item\textbf{Bipartite graph:} it is one where the node set $V$ can be partitioned into sets $X$ and $Y$ in such a way that every edge has one end in $X$ and the other end in $Y$
\item If a graph $G$ is bipartite, then it cannot contain an odd cycle.
\item To determine whether $G$ is bipartite, we can also show that we can find an odd cycle in $G$ whenever it is not bipartite.
\item We perform BFS, colors $s$ red, all of later $L_1$ blue, all of layer $L_2$ red, and so on - coloring odd-numbered layers blue and even-numbered layers red. We implement this by adding an extra array $Color$ over the nodes. Whenever we get to a step in BFS where we are adding a node $v$ to a list $L[i+1]$, we assign $Color[v] = red$ if $i+1$ is an even number and $Color[v] = blue$ if $L[i+1]$ is an odd number. At the end of the procedure we scan all the edges and determine whether there is any edge for which both ends received the same color. Thus the coloring algorithm is $O(m+n)$, as it is for BFS.

\item Let $G$ be a connected graph, and let $L_1, L_2,\ldots$ be the layers produced by BFS starting at node $s$. Then exactly one of the following two things must hold:
    \begin{itemize}
        \item There is no edge of $G$ joining two nodes of the same layer. In this case $G$ is a bipartite graph in which the nodes in even-numbered layers can be colored red, and the nodes in odd-numbered layers can be colored blue.
        \item There is an edge of $G$ joining two nodes of the same layer. In this case, $G$ contains an odd-length cycle, and so it cannot be bipartite.
    \end{itemize}
\end{itemize}
\end{homeworkProblem}
\pagebreak

\begin{homeworkProblem}
\textbf{Data Structures Review}

\underline{\textbf{Data Structures Overview}}
\begin{enumerate}
    \item Array
    \begin{itemize}
        \item Store homogeneous elements at contiguous locations
        \item Size of an array must be provided before storing data. Let size be $n$.
        \item Accessing Time: $O(1)$ [because elements are stored at contiguous locations]
        \item Search Time: $O(n)$ for Sequential Search; $O(Log(n))$ for Binary Search [If sorted]
        \item Insertion Time: $O(n)$ [Worst case if inserted at the beginning of array - shifting all of the elements]
        \item Deletion Time: $O(n)$ [Worst case deleting at the beginning of the array - shifting all of the elements]
    \end{itemize}
    
    \item Linked List
    \begin{itemize}
        \item Each element is a separate object. Each element contains the data and a reference to the next node.
        \item\textbf{Singly Linked List: }Every node stores reference of next node in list. Last node has reference NULL.
        \item\textbf{Doubly Linked List: }Two references associated with each node. One reference points to the next node and one points to the previous node. can traverse in both direction. NULL at the end.
        \item\textbf{Circular Linked List: }All nodes are connected to form a circle. No NULL at the end. Can be singly or doubly linked. Any node can be the starting node.
        \item Accessing time of an element : $O(n)$
        \item Search time of an element : $O(n)$
        \item Insertion of an Element : $O(1)$ [If we are at the position where we have to insert an element] 
        \item Deletion of an Element : $O(1)$ [If we know address of node previous to the node to be deleted] 
    \end{itemize}
    
    \item Stack
    \begin{itemize}
        \item Last in, first out (LIFO) 
        \item Abstract data type as a collection of elements
        \item Operations: push (adds an element to collection), pop (removes the last element that was added) - both occur at the top of the stack.
        \item Can be implemented as array or linked list
        \item Insertion : $O(1)$
        \item Deletion :  $O(1)$
        \item Access Time : $O(n)$ [Worst Case] Insertion and Deletion are allowed on one end. 
    \end{itemize}
    
    \item Queue
    \begin{itemize}
        \item First in, first out (FIFO)
        \item Abstract data type as a collection of elements
        \item Operations: enqueue (adds an element to the end of collection), dequeue (removes the first element that was added).
        \item Can be implemented as array or linked list    
        \item Insertion : $O(1)$
        \item Deletion :  $O(1)$
        \item Access Time : $O(n)$ [Worst Case] Insertion and Deletion are allowed on one end. 
    \end{itemize}

    \item Binary Tree
    \begin{itemize}
        \item Hierarchical data structure
        \item Each node has at most two children: left child and right child
        \item Usually implemented with Linked Lists, but possible with arrays too
        \item Represented by a pointer to root. If tree is empty, root = NULL.
        \item A node contains: Data, pointer to left child, and pointer to right child
        \item Traversal: 
        \begin{enumerate}
            \item Depth First Traversal: Inorder (Left-Root-Right), Preorder (Root-Left-Right) and Postorder (Left-Right-Root)
            \item Breadth First Traversal: Level Order Traversal
        \end{enumerate}
        \item The maximum number of nodes at level $l = 2^{l-1}$.
        \item Maximum number of nodes = $2^h – 1$, where $h$ is the height of the tree (Max. number of nodes on root to leaf path)
        \item Minimum possible height =  $ceil(Lg(n+1))$ (number of leaf nodes is always one more than nodes with two children)
        \item Time Complexity of Tree Traversal: O(n) - consider degenerate trees
    \end{itemize}
    
    \item Binary Search Tree    
    \begin{itemize}
        \item A Binary tree with three additional properties:
        \begin{itemize}
            \item Left subtree of a node contains only nodes with keys less than the node's key
            \item The right subtree of a node contains only nodes with keys greater than the node's key.
            \item The left and right subtree each must also be a binary search tree.
        \end{itemize}
        \item Search : $O(h)$
        \item Insertion : $O(h)$
        \item Deletion : $O(h)$
        \item Extra Space : $O(n)$ for pointers
        \item $h$: Height of BST and $n$: Number of nodes in BST
        \item If Binary Search Tree is height balanced, then $h = O(Log(n))$ \item Self-Balancing BSTs such as AVL Tree, Red-Black Tree and Splay Tree make sure that height of BST remains $O(Log(n))$
        \item BST provide moderate access/search (quicker than Linked List and slower than arrays).
        \item BST provide moderate insertion/deletion (quicker than Arrays and slower than Linked Lists).
    \end{itemize}
    
    \item Binary Heap
    \begin{itemize}
        \item A Binary tree with two additional properties:
        \begin{itemize}
            \item It's a complete tree (All levels are completely filled except possibly the last level and the last level has all keys as left as possible). This property of Binary Heap makes them suitable to be stored in an array.
            \item A Binary Heap is either Min Heap or Max Heap. In a Min Binary Heap, the key at root must be minimum among all keys present in Binary Heap. The same property must be recursively true for all nodes in Binary Tree. Max Binary Heap is similar to Min Heap. It is mainly implemented using array.
        \end{itemize}
        \item Get Minimum in Min Heap: $O(1)$ [Or Get Max in Max Heap]
        \item Extract Minimum Min Heap: $O(Log(n))$ [Or Extract Max in Max Heap]
        \item Decrease Key in Min Heap: $O(Log(n))$  [Or Increase key in Max Heap]
        \item Insert: $O(Log(n))$
        \item Delete: $O(Log(n))$
    \end{itemize}
    
    \item Hashing
    \begin{itemize}
        \item\textbf{Hash function: }A function that converts a given big input key to a small practical integer value. The mapped integer value is used as an index in hash table.
        \item Usually efficiently computable and uniformly distributes the keys (each table position equally likely for each key)
        \item\textbf{Hash table: }An array that stores pointers to records corresponding to a given phone number. An entry in hash table is NIL if no existing phone number has hash function value equal to the index for the entry.
        \item\textbf{Collision Handling: }Since a hash function gets us a small number for a key which is a big integer or string, there is possibility that two keys result in same value. The situation where a newly inserted key maps to an already occupied slot in hash table is called collision and must be handled using some collision handling technique.
        \item\textbf{Chaining:} The idea is to make each cell of hash table point to a linked list of records that have same hash function value. Chaining is simple, but requires additional memory outside the table.
        \item\textbf{Open Addressing:} In open addressing, all elements are stored in the hash table itself. Each table entry contains either a record or NIL. When searching for an element, we one by one examine table slots until the desired element is found or it is clear that the element is not in the table.
        \item Space: $O(n)$
        \item Search: $O(1)$ [Average]; $O(n)$ [Worst case]
        \item Insertion: $O(1)$ [Average]; $O(n)$ [Worst Case]
        \item Deletion: $O(1)$ [Average]; $O(n)$ [Worst Case]
        \item Hashing seems better than BST for all the operations. But in hashing, elements are unordered and in BST elements are stored in an ordered manner. Also BST is easy to implement but hash functions can sometimes be very complex to generate. In BST, we can also efficiently find floor and ceiling of values.
    \end{itemize}
    
    \item Graph
    \begin{itemize}
        \item Time Complexities in case of Adjacency Matrix:
        \begin{itemize}
            \item Traversal: (By BFS or DFS) $O(V^2)$
            \item Space: $O(V^2)$
        \end{itemize}
        \item Time Complexities in case of Adjacency List :
        \begin{itemize}
            \item Traversal: $O(|E|log(|V|))$ (By BFS or DFS) 
            \item Space: $O(|V|+|E|)$
        \end{itemize}
    \end{itemize}
    
\end{enumerate}
\end{homeworkProblem}
\pagebreak

\end{document}